# Model Parameters Summary

## ğŸ“ Configuration:

```bash
python main_jax.py \
    --model vfsddpm_jax \
    --dataset cifar100 \
    --sample_size 6 \
    --image_size 32 \
    --patch_size 2 \
    --batch_size 32 \
    --hidden_size 384 \
    --depth 12 \
    --num_heads 6 \
    --mlp_ratio 4.0 \
    --hdim 384 \              # After fix (was 256)
    --context_channels 384    # After fix (was 256)
```

---

## ğŸ¯ **TOTAL MODEL PARAMETERS: 43.5M**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TOTAL: 43,509,900 params (43.5M)     â”‚
â”‚                                        â”‚
â”‚  â”œâ”€ DiT (Generator):    32.7M (75.2%) â”‚
â”‚  â””â”€ Encoder (Context):  10.8M (24.8%) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š Detailed Breakdown:

### **1. DiT (Diffusion Transformer) - 32.7M params**

| Component | Params | Details |
|-----------|--------|---------|
| **Patch Embedding** | 4,608 | Linear: 12 â†’ 384 |
| **Position Embedding** | 98,304 | 256 patches Ã— 384 |
| **Time Embedding** | 245,760 | MLP: 256 â†’ 384 â†’ 384 |
| **Context Projection** | 147,840 | Dense: 384 â†’ 384 âœ… |
| **DiT Blocks (Ã—12)** | 31,938,048 | **2.66M per block** |
| **Final Layer** | 301,068 | Norm + adaLN + Linear |
| **TOTAL** | **32,735,628** | **~32.7M** |

#### **Per DiT Block (2.66M params):**
- LayerNorm (Ã—2): 1,536
- Self-Attention: 591,360
  - QKV projection: 443,520
  - Output projection: 147,840
- MLP: 1,181,568
  - FC1 (384â†’1536): 591,360
  - FC2 (1536â†’384): 590,208
- adaLN-Zero (conditioning): 887,040

---

### **2. Encoder (sViT) - 10.8M params**

| Component | Params | Details |
|-----------|--------|---------|
| **SPT Embedding** | 28,032 | Project: 72 â†’ 384 |
| **Position Embedding** | 98,688 | 257 positions Ã— 384 |
| **Transformer Blocks (Ã—6)** | 10,646,784 | **1.77M per block** |
| **Final LayerNorm** | 768 | Norm before output |
| **TOTAL** | **10,774,272** | **~10.8M** |

#### **Per Encoder Block (1.77M params):**
- LayerNorm (Ã—2): 1,536
- Self-Attention: 591,360
- MLP: 1,181,568

---

## ğŸ“ˆ Comparison: Before vs After Fix

| Aspect | Before (hdim=256) | After (hdim=384) | Change |
|--------|-------------------|------------------|--------|
| **Encoder** | 4.8M | 10.8M | +6.0M (+124%) |
| **DiT** | 32.7M | 32.7M | +0.05M (+0.2%) |
| **Context Projection** | 98,688 | 147,840 | +49,152 (+50%) |
| **TOTAL** | **37.5M** | **43.5M** | **+6.0M (+16%)** |

### **Analysis:**

âœ… **Encoder increase dominates:**
- hdim 256â†’384 increases encoder by **+124%**
- But encoder is only ~25% of total model
- â†’ Overall increase: **+16%** (acceptable!)

âœ… **DiT barely changes:**
- DiT uses `hidden_size=384` (always)
- Only context projection changes (256â†’384)
- â†’ DiT increase: **+0.2%** (negligible)

âœ… **Context projection:**
- Before: 256 Ã— 384 = 98,688 params
- After: 384 Ã— 384 = 147,840 params
- â†’ +50% params for this layer
- But this is critical for avoiding dimension mismatch! âœ…

---

## ğŸ’¾ Memory Estimates:

### **Training (float32):**

```
Model params:           43.5M Ã— 4 bytes = 174 MB
Optimizer states (Adam): 43.5M Ã— 8 bytes = 348 MB  (m, v)
Gradients:              43.5M Ã— 4 bytes = 174 MB
EMA params:             43.5M Ã— 4 bytes = 174 MB

Subtotal:                              ~870 MB

Activations (batch_size=32, depth=12):
  DiT activations:      ~2-3 GB (estimate)
  Encoder activations:  ~500 MB (estimate)

TOTAL TRAINING MEMORY: ~4-5 GB per device
```

### **Inference (float32):**

```
Model params:           174 MB
Activations:            ~500 MB (batch_size=16)

TOTAL INFERENCE MEMORY: ~700 MB per device
```

---

## ğŸ¯ Key Takeaways:

### **1. Model Size:**
- **43.5M params total** - Medium-sized model
- DiT dominates: **75%** of params (32.7M)
- Encoder: **25%** of params (10.8M)

### **2. After Dimension Fix:**
- **+16% params** (37.5M â†’ 43.5M)
- Mostly from encoder (hdim 256â†’384)
- **Trade-off is worth it:**
  - âœ… No information bottleneck
  - âœ… No expansion artifacts (256â†’384)
  - âœ… Better context quality
  - âœ… Expected better FID

### **3. Comparison to Other Models:**

| Model | Params | Notes |
|-------|--------|-------|
| **VFSDDPM (ours)** | **43.5M** | DiT-based, few-shot |
| DiT-S/2 | 33M | Single-class, no context |
| DiT-B/2 | 130M | Larger backbone |
| U-Net (DDPM) | 35M | CNN-based |
| Stable Diffusion | 860M | Text-to-image |

â†’ Our model is **reasonably sized** for few-shot generation!

### **4. Computational Cost:**

**Per training step (batch_size=32):**
- Forward pass: ~100-150ms (GPU)
- Backward pass: ~200-300ms (GPU)
- Total: ~300-450ms per step

**Full training (200k steps):**
- Time: ~17-25 hours (1 GPU)
- On Kaggle (9h limit): ~72k-108k steps

### **5. Scaling Options:**

**If too large:**
- âœ… Reduce `depth`: 12 â†’ 8 (saves ~10M params)
- âœ… Reduce `hidden_size`: 384 â†’ 256 (saves ~15M params)
- âœ… Reduce encoder depth: 6 â†’ 4 (saves ~3.5M params)

**If want larger:**
- âœ… Increase `depth`: 12 â†’ 16 (adds ~10M params)
- âœ… Increase `hidden_size`: 384 â†’ 512 (adds ~30M params)
- âš ï¸ Don't reduce `hdim` below 384 (dimension mismatch issue!)

---

## ğŸ“ Summary Table:

| Component | Params | % of Total | Key Feature |
|-----------|--------|------------|-------------|
| **DiT Blocks** | 31.9M | 73.4% | Main generator |
| **Encoder Blocks** | 10.6M | 24.5% | Context extraction |
| **Embeddings** | 0.5M | 1.2% | Patch + Position |
| **Projections** | 0.4M | 0.9% | Time + Context |
| **TOTAL** | **43.5M** | **100%** | Few-shot diffusion |

---

## âœ… Conclusion:

**43.5M parameters** vá»›i config nÃ y:
- âœ… Reasonable size cho few-shot learning
- âœ… Balance tá»‘t giá»¯a encoder (25%) vÃ  generator (75%)
- âœ… Dimension fix (+16% params) Ä‘Ã¡ng giÃ¡ cho quality
- âœ… Fit trong Kaggle memory (4-5GB training)
- âœ… Train Ä‘Æ°á»£c trong 9h Kaggle limit (~80k steps)

**Recommendation: Config nÃ y á»•n! CÃ³ thá»ƒ báº¯t Ä‘áº§u training!** ğŸš€
