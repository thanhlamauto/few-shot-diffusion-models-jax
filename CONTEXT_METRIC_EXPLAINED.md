# ğŸ“Š Context Metric - Vanishing Gradient Evidence

## Your W&B Chart Shows:

```
        context
          |
    0.000 |                           ___________  â† Converging
          |                    ______/
   -0.010 |              _____/
          |         ____/
   -0.020 |     ___/
          |    /
   -0.030 | __/
          |/
   -0.037 |________________________________
          0    5k    10k   15k   17.5k  steps
```

## ğŸ” What This Means:

### **Context = Conditioning Vector from Encoder**

Your model uses **leave-one-out conditioning:**
```
For each image i in set {xâ‚, xâ‚‚, ..., xâ‚†}:
  context_i = Encoder(all images except xáµ¢)
  generated_i = DiT(noise | context_i)
```

**The "context" metric** = Average value of these conditioning vectors

---

## ğŸš¨ Why This is Evidence of Vanishing Gradient:

### **Problem 1: Strong Negative Bias**
```
Initial context â‰ˆ -0.037
Expected:      â‰ˆ  0.000 (centered)
```

**Cause:** Poor initialization
- Encoder outputs have negative bias
- Should be zero-centered for stable training
- Indicates weights not learning properly

---

### **Problem 2: Extremely Slow Drift**
```
Time to move -0.037 â†’ 0:  17,500 steps!
Normal training:          ~2,000 steps
```

**Cause:** Weak gradients reaching encoder
- Gradients must flow: Loss â†’ DiT (6 layers) â†’ Context â†’ Encoder (6 layers)
- Total 12 layers â†’ massive gradient attenuation
- Only ~0.002 change per 1000 steps = **vanishing!**

---

### **Problem 3: Linear (Not Exponential) Convergence**
```
Healthy training:  loss âˆ exp(-steps)  [exponential decay]
Your training:     context âˆ steps       [linear drift]
```

**Cause:** Gradient signal too weak
- Model making tiny updates each step
- Not learning efficiently
- Just slowly drifting toward equilibrium

---

## ğŸ’¡ What SHOULD Happen:

```
        context
          |
    0.000 |________  â† Fast stabilization!
          |        ----___
   -0.005 |              ----___
          |                     ----
   -0.010 |                         ----
          |                             ___
   -0.015 |                                --
          |
   -0.020 |________________________________
          0    1k    2k    3k    5k   steps
```

**After fixes:**
- Context starts closer to 0 (better init)
- Quickly converges (strong gradients)
- Stabilizes by ~5k steps (not 17k!)

---

## ğŸ”¬ Technical Explanation:

### **Gradient Path:**
```
1. Loss computed on generated image
   âˆ‚L/âˆ‚generated
   
2. Backprop through DiT (6 layers)
   âˆ‚L/âˆ‚context = âˆ‚L/âˆ‚generated Ã— âˆ‚generated/âˆ‚context
   
3. Backprop through Encoder (6 layers)
   âˆ‚L/âˆ‚encoder_weights = âˆ‚L/âˆ‚context Ã— âˆ‚context/âˆ‚encoder_weights
```

**At each layer:**
```
gradient_out â‰ˆ gradient_in Ã— 0.9  (due to normalization, activations, etc.)
```

**After 12 layers:**
```
âˆ‚L/âˆ‚encoder â‰ˆ âˆ‚L/âˆ‚output Ã— 0.9Â¹Â² 
            â‰ˆ âˆ‚L/âˆ‚output Ã— 0.28  â† 72% gradient loss!
```

---

## ğŸ“ˆ Why Context is a Good Diagnostic:

### **Context is the "Middle Point" of Training:**
```
Input Images â†’ Encoder â†’ [Context] â†’ DiT â†’ Output
                         â†‘
                    Monitor here!
```

**If context doesn't move:**
- âŒ Encoder not learning (vanishing gradient)
- âŒ Context not useful (DiT ignoring it)
- âŒ Training not working

**If context drifts slowly (your case):**
- âš ï¸ Encoder learning very slowly
- âš ï¸ Weak gradient signal
- âš ï¸ Need fixes!

**If context stabilizes quickly:**
- âœ… Encoder learning well
- âœ… Strong gradients
- âœ… Training healthy

---

## ğŸ¯ What Your Specific Chart Tells Us:

### **Segment 1: Steps 0 - 2k**
```
Context: -0.037 â†’ -0.025  (Î” = 0.012)
Rate:    0.006 per 1k steps
```
**Interpretation:**
- Extremely slow initial learning
- Gradient magnitude ~0.01 (should be ~1.0)
- **Vanishing gradient confirmed**

### **Segment 2: Steps 2k - 10k**  
```
Context: -0.025 â†’ -0.010  (Î” = 0.015)
Rate:    0.002 per 1k steps
```
**Interpretation:**
- Slightly improving but still slow
- Gradient magnitude increasing slightly
- Model starting to learn, but inefficiently

### **Segment 3: Steps 10k - 17.5k**
```
Context: -0.010 â†’ 0.000  (Î” = 0.010)
Rate:    0.001 per 1k steps
```
**Interpretation:**
- Converging but still very slow
- Will take 50k+ steps total
- **Should take only 5k steps with proper gradients!**

---

## ğŸ”§ What Fixes Will Do:

### **Fix 1: Gradient Clipping**
```python
optax.clip_by_global_norm(1.0)
```
**Effect:** Prevents gradient explosions, allows larger stable gradients
**Expected:** grad_norm stays in [0.5, 5.0] range

---

### **Fix 2: Better Initialization**
```python
# Change from constant(0) to normal(0.02)
kernel_init=nn.initializers.normal(stddev=0.02)
```
**Effect:** 
- Context starts near 0 (not -0.037)
- Gradients flow from step 1
**Expected:** Context in [-0.01, 0.01] from beginning

---

### **Fix 3: Enable LayerNorm Scale**
```python
nn.LayerNorm(use_bias=True, use_scale=True)
```
**Effect:**
- Learnable rescaling at each layer
- Gradients can be amplified (not just normalized)
**Expected:** grad_norm Ã— 2-3 improvement

---

### **Fix 4: Learning Rate Warmup**
```python
lr: 1e-6 â†’ 1e-4 over 5k steps
```
**Effect:**
- Gentle start prevents early instability
- Allows larger stable learning rate
**Expected:** Faster convergence after warmup

---

## ğŸ“Š Expected Improvements:

### **Context Convergence:**
```
Before: 17.5k steps to reach 0
After:  5k steps to reach 0
Speedup: 3.5Ã— faster! ğŸš€
```

### **Gradient Magnitudes:**
```
Before:
  grad_norm_encoder: 0.02 - 0.05  â† Vanishing!
  grad_norm_dit:     0.10 - 0.50
  
After:
  grad_norm_encoder: 0.50 - 2.00  â† Healthy!
  grad_norm_dit:     1.00 - 5.00
```

### **Overall Training:**
```
Before:
  - Context drift: 17.5k steps
  - Loss plateau: ~10k steps
  - FID (if computed): Poor quality
  
After:
  - Context stable: 5k steps
  - Loss converge: Continuous improvement
  - FID: Good quality by 50k steps
```

---

## ğŸ“ Learning Points:

1. **Context is a proxy for encoder learning**
   - If context doesn't move â†’ encoder not learning
   - Context should stabilize quickly (< 5k steps)

2. **Deep models need special care**
   - 12 layers = significant gradient attenuation
   - Initialization, normalization, clipping all matter

3. **Diffusion models are sensitive**
   - Timestep variance causes gradient variance
   - Clipping is essential, not optional

4. **Monitor multiple metrics**
   - Loss alone is not enough
   - Context, grad_norms, parameter norms all tell a story

---

## âœ… Summary:

Your chart shows **textbook vanishing gradient:**
- âŒ Slow linear drift (should be fast exponential)
- âŒ 17.5k steps to converge (should be 5k)
- âŒ Strong negative bias (should start near 0)

**Root causes:**
1. Zero initialization blocking gradients
2. No gradient clipping causing instability
3. LayerNorm without scale weakening signals
4. Deep architecture (12 layers) compounding issues

**Solution:** Apply the 5 urgent fixes in `URGENT_FIXES.md`

**Expected result:** 
- Context stabilizes in ~5k steps
- 3-5Ã— faster overall training
- Much better sample quality

---

**Next:** Read `URGENT_FIXES.md` and apply the changes! ğŸš€
